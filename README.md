# ğŸ§  TANGNET - Decentralized AI Cluster

<div align="center">
  <img src="picluster.png" alt="TANGNET Architecture" width="600">
  
  [![Phase](https://img.shields.io/badge/Phase-1%20of%205-blue)](https://github.com/bneidlinger/tangent)
  [![Model](https://img.shields.io/badge/Model-TinyLlama%201.1B-green)](https://github.com/bneidlinger/tangent)
  [![Nodes](https://img.shields.io/badge/Current%20Nodes-1-orange)](https://github.com/bneidlinger/tangent)
  [![Target](https://img.shields.io/badge/Target%20Nodes-100-red)](https://github.com/bneidlinger/tangent)
  
  **Building a 100-node Raspberry Pi cluster for distributed AI inference**
</div>

## ğŸŒŸ Overview

TANGNET (Tangential Network) is an ambitious project to create a fully offline-capable AI infrastructure combining edge computing with powerful GPU acceleration. No cloud dependencies, complete autonomy.

### ğŸ¯ Project Goals

- **100 Raspberry Pi 5 units** (8GB) for distributed edge computing
- **10 RTX 5090 GPU rigs** for heavy inference tasks  
- Voice transcription, document retrieval, and AI inference
- Completely self-contained synthetic intelligence ecosystem
- Zero reliance on external APIs or cloud services

## ğŸš€ Quick Start

### Access the Current Node

```bash
# SSH into the primary Pi
ssh brand@192.168.1.31

# Run inference using the tangnet alias
tangnet "What is the meaning of life?"
```

### Launch the Chat Interface

```bash
cd tanchat
pip install -r requirements.txt
python chatapi.py
```

Visit `http://localhost:8000` for the web interface.

## ğŸ“Š Current Progress

### âœ… Phase 1: Single Node (Complete)
- Raspberry Pi 5 (8GB) operational
- TinyLlama 1.1B model deployed
- ~10-15 tokens/second inference
- Custom chat API with WebSocket support
- SSH/VNC remote access configured

### ğŸ”„ Phase 2: Mini Cluster (In Progress)
- Adding 4 additional Pi nodes
- Implementing job distribution
- Building monitoring dashboard
- Integrating RTX 3080 GPU server

## ğŸ—ï¸ Architecture

```
TANGNET Infrastructure
â”œâ”€â”€ Edge Nodes (Raspberry Pi 5)
â”‚   â”œâ”€â”€ Inference Engine (llama.cpp)
â”‚   â”œâ”€â”€ Local Model Storage
â”‚   â””â”€â”€ Mesh Networking (Tailscale)
â”œâ”€â”€ GPU Acceleration Layer
â”‚   â”œâ”€â”€ RTX 3080 Server (Phase 2)
â”‚   â””â”€â”€ RTX 5090 Rigs (Phase 5)
â””â”€â”€ Control Plane
    â”œâ”€â”€ FastAPI Chat Server
    â”œâ”€â”€ WebSocket Real-time Streaming
    â””â”€â”€ Distributed Job Queue
```

## ğŸ› ï¸ Technical Stack

### Hardware
- **Current**: 1x Raspberry Pi 5 (8GB RAM)
- **Phase 5**: 100x Pi 5 + 10x RTX 5090 rigs
- **Networking**: Gigabit Ethernet mesh
- **Storage**: Distributed across nodes

### Software
- **Inference**: llama.cpp (ARM-optimized)
- **API**: FastAPI + WebSockets
- **Models**: TinyLlama 1.1B (expanding to Llama 3, Mixtral)
- **Mesh**: Tailscale VPN
- **OS**: Raspberry Pi OS Lite

## ğŸ“ˆ Performance Metrics

| Metric | Current | Target (Phase 5) |
|--------|---------|------------------|
| Nodes | 1 | 100 |
| Models | TinyLlama 1.1B | Multiple 70B+ |
| Inference Speed | 10-15 tok/s | 1000+ tok/s |
| Availability | Single node | 99.9% uptime |
| Power Draw | ~10W | ~5kW total |

## ğŸ—ºï¸ Roadmap

<details>
<summary><b>Phase 2: 5-Node Mini Cluster</b></summary>

- [ ] Deploy 4 additional Pi nodes
- [ ] Implement basic load balancing
- [ ] Add RTX 3080 GPU server
- [ ] Create monitoring dashboard
- [ ] Test multi-node inference

</details>

<details>
<summary><b>Phase 3: 10-Node AI Stack</b></summary>

- [ ] Distributed vector database (ChromaDB)
- [ ] Job queue management system
- [ ] Persistent conversation memory
- [ ] API gateway with auth
- [ ] Automated model deployment

</details>

<details>
<summary><b>Phase 4: 25-Node Regional Cluster</b></summary>

- [ ] Zone-based architecture
- [ ] Redundancy and auto-failover
- [ ] Advanced load distribution
- [ ] Real-time performance monitoring
- [ ] Voice command integration

</details>

<details>
<summary><b>Phase 5: 100-Node TANGNET GRIDCORE</b></summary>

- [ ] Full 100 Pi deployment
- [ ] 10x RTX 5090 GPU rigs
- [ ] Cluster-wide message bus
- [ ] Complete offline operation
- [ ] Multi-modal AI capabilities

</details>

## ğŸ“š Documentation

- [Project Overview](overview/index.html) - Vision and goals
- [Setup Guide](overview/guide.html) - Step-by-step Pi configuration
- [Hardware Specs](overview/materials.html) - Complete parts list
- [API Documentation](tanchat/README.md) - Chat server API reference
- [Mesh Network](mesh%20docs/mesh_network.md) - Node inventory and topology

## ğŸ¤ Contributing

While this is currently a personal project, contributions are welcome! Areas of interest:

- Distributed inference algorithms
- Web UI improvements
- Performance optimizations
- Documentation and tutorials
- Hardware recommendations

## ğŸ“œ License

This project is open source. License TBD.

## ğŸ‘¨â€ğŸ’» Creator

Built by **Brandon** - Powered by curiosity, caffeine, and the relentless pursuit of decentralized AI.

---

<div align="center">
  <i>"TANGNET isn't just a cluster â€” it's a bunker-grade synthetic brain."</i>
  
  â­ Star this repo to follow the journey from 1 to 100 nodes!
</div>