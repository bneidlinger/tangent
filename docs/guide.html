<!DOCTYPE html>

<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Tangnet LLM Pi Guide</title>
  <style>
    body {
      background-color: #0d0d0d;
      color: #33ff33;
      font-family: 'Courier New', Courier, monospace;
      padding: 20px;
    }
    h1, h2 {
      color: #39ff14;
    }
    .section {
      margin-bottom: 40px;
    }
    .cmd {
      background-color: #1a1a1a;
      padding: 10px;
      border-left: 5px solid #39ff14;
      margin: 10px 0;
      white-space: pre-wrap;
    }
    a {
      color: #00ffff;
    }
  </style>
</head>
<body>
  <h1>üß™ Tangnet Local LLM Node Guide</h1>
  <div class="section">
    <h2>üñ•Ô∏è System Overview</h2>
    <ul>
      <li><strong>Raspberry Pi 5 (8GB)</strong> ‚Äî Running TinyLlama LLM</li>
      <li><strong>Network Address:</strong> <code>192.168.1.31</code></li>
      <li><strong>Laptop:</strong> RTX 4060, 32GB RAM, Windows</li>
      <li><strong>Main Rig:</strong> RTX 3070, 64GB RAM, Windows</li>
    </ul>
  </div>

  <div class="section">
    <h2>üß† LLM Runtime (TinyLlama)</h2>
    <p>Run the model:</p>
    <div class="cmd">
~/tangnet/llama.cpp/build/bin/llama-run ~/tangnet/llama.cpp/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf "Your prompt here"
    </div>
    <p>Create a bash alias:</p>
    <div class="cmd">
echo "alias tangnet='~/tangnet/llama.cpp/build/bin/llama-run ~/tangnet/llama.cpp/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf'" >> ~/.bashrc && source ~/.bashrc
    </div>
    <p>Then just run:</p>
    <div class="cmd">
tangnet "What's the mission?"
    </div>
  </div>

  <div class="section">
    <h2>üîê Connecting to Pi</h2>
    <p>From laptop or desktop:</p>
    <div class="cmd">
ssh brand@192.168.1.31
    </div>
    <p>Transfer a file from laptop to Pi:</p>
    <div class="cmd">
scp path/to/your/model.gguf brand@192.168.1.31:~/tangnet/llama.cpp/
    </div>
    <p>Use VNC Viewer (if enabled):</p>
    <div class="cmd">
vncviewer 192.168.1.31:1
    </div>
  </div>

  <div class="section">
    <h2>üîß Managing the Pi</h2>
    <p>Start, stop, or reboot properly:</p>
    <div class="cmd">
sudo shutdown -h now    # Shutdown safely
sudo reboot             # Reboot the Pi
    </div>
    <p>Check CPU temperature:</p>
    <div class="cmd">
vcgencmd measure_temp
    </div>
    <p>Monitor processes:</p>
    <div class="cmd">
htop
    </div>
    <p><strong>Best practice:</strong> Shutdown the Pi if not used daily. Leave on for automation or persistent use.</p>
  </div>

  <div class="section">
    <h2>üß™ Handling the Beast</h2>
    <p>This Pi is now your edge-node for LLM inference. Treat it with respect. Don‚Äôt yank the power ‚Äî always shut down cleanly. For now, you can power it on when needed, SSH/VNC in, and fire up the llama.</p>
    <p><strong>Future plans:</strong> hook it into a smart outlet and run a local API!</p>
  </div>

  <div class="section">
    <h2>üß† Rick Note:</h2>
    <p>"Morty, we got a local brain running on a Pi, and it‚Äôs not even melting, Morty. That‚Äôs like fitting a black hole in a lunchbox. Don‚Äôt screw this up."</p>
  </div>
</body>
</html>
