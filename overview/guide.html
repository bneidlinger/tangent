<!DOCTYPE html>

<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tangnet Pi Node - User Guide</title>
  <style>
    body {
      background-color: #0b0c10;
      color: #00ff90;
      font-family: 'Courier New', Courier, monospace;
      padding: 1.5rem;
    }
    h1, h2, h3 {
      color: #00ffe5;
    }
    code {
      background: #1f1f1f;
      color: #00ffaa;
      padding: 0.3rem;
      border-radius: 4px;
      display: block;
      margin: 0.3rem 0;
    }
    pre {
      background: #1f1f1f;
      color: #00ffaa;
      padding: 1rem;
      border-radius: 5px;
      overflow-x: auto;
    }
    a {
      color: #ff70d4;
      text-decoration: none;
    }
  </style>
</head>
<body>
  <h1>&#128125; Tangnet Pi Node User Guide</h1>
  <p>This guide walks through setup and use of your Raspberry Pi node (nicknamed <strong>Tangnet Node 01</strong>) for local LLM testing. You're running this on a Raspberry Pi 5 with 8GB RAM. Also in the mix:</p>
  <ul>
    <li><strong>Main Rig</strong>: RTX 3070, 64GB RAM, Windows</li>
    <li><strong>Laptop</strong>: RTX 4060, 32GB RAM, Ryzen AI 9 HX 370, Windows</li>
  </ul>

  <h2>&#9889; Setup Checklist</h2>
  <ul>
    <li>✔ Pi powered via CanaKit with working mouse/keyboard and HDMI</li>
    <li>✔ Wifi configured via <code>wpa_supplicant.conf</code> (optional: ethernet)</li>
    <li>✔ SSH, VNC, and local terminal access all working</li>
    <li>✔ Python 3 and virtualenv set up</li>
    <li>✔ llama.cpp cloned and built with <code>cmake .. && make -j4</code></li>
  </ul>

  <h2>&#129504; Useful Commands</h2>
  <h3>Model Download (Huggingface CLI)</h3>
  <pre><code>source ~/hfenv/bin/activate
python -m huggingface_hub download TheBloke/TinyLlama-1.1B-Chat-GGUF tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --local-dir ~/tangnet/llama.cpp</code></pre>

  <h3>Run LLaMA Model</h3>
  <pre><code>~/tangnet/llama.cpp/build/bin/llama-run ~/tangnet/llama.cpp/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf "Your prompt here"</code></pre>

  <h3>Optional Alias (Add to ~/.bashrc)</h3>
  <pre><code>alias tangnet='~/tangnet/llama.cpp/build/bin/llama-run ~/tangnet/llama.cpp/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf'</code></pre>

  <h2>&#128679; Troubleshooting</h2>
  <ul>
    <li>If model won’t load, check it with <code>head -c 4 filename.gguf</code> (should start with <code>GGUF</code>)</li>
    <li>If llama-run fails, confirm build finished without errors and you're in the right path</li>
    <li>Clear failed files with <code>rm filename.gguf</code> and re-download</li>
  </ul>

  <h2>&#128736; Tips</h2>
  <ul>
    <li>Keep models under 2GB for Pi testing or you'll melt the RAM</li>
    <li>Use <code>htop</code> or <code>top</code> to monitor usage in real-time</li>
    <li>Add more swap if building large C++ projects on Pi</li>
    <li>Keep the Pi cool with a fan from your CanaKit!</li>
  </ul>

  <p>More to come in the next phase, Morty! We’ll be adding UI overlays, server endpoints, and maybe a multiverse interface for AI bots. Stay squanchy!</p>
</body>
</html>
