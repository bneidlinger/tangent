<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Llama 2 7B Setup on Pi 5 (16GB) — Tangnet Mesh Guide</title>
  <link href="https://fonts.googleapis.com/css2?family=Fira+Mono:wght@400;700&display=swap" rel="stylesheet">
  <style>
    body {
      background: #171a22;
      color: #b5ffcb;
      font-family: 'Fira Mono', monospace;
      padding: 0 0 3em 0;
      margin: 0;
    }
    header {
      background: #22282e;
      padding: 1.3em 0 0.6em 0;
      text-align: center;
      color: #6ae67a;
      font-size: 2.2em;
      font-weight: bold;
      border-bottom: 4px solid #37ff79;
      letter-spacing: 2px;
      box-shadow: 0 5px 22px #36ffb26e;
    }
    h2 {
      color: #3bff39;
      border-bottom: 1.5px solid #59ffaa;
      padding-bottom: 3px;
      margin-top: 2.3em;
    }
    .section {
      background: #202c22c0;
      border-left: 6px solid #36ff79;
      box-shadow: 0 4px 22px #36ffb244;
      padding: 1.5em 2em;
      margin: 2em auto 0 auto;
      max-width: 750px;
      border-radius: 14px;
      font-size: 1.09em;
    }
    pre, code {
      background: #212944;
      color: #a2e3d2;
      border-radius: 6px;
      padding: 0.2em 0.7em;
      font-family: 'Fira Mono', monospace;
      font-size: 1.08em;
      line-height: 1.6em;
      overflow-x: auto;
    }
    .tip {
      color: #84ffb9;
      background: #2e5442;
      border-left: 3px solid #3bff39;
      padding: 0.4em 1.2em;
      margin: 1em 0 0.7em 0;
      border-radius: 5px;
      font-size: 1.02em;
      box-shadow: 0 1px 5px #32d67122;
    }
    ul, ol { font-size: 1.06em; }
    a, a:visited { color: #36ff79; }
    .footer {
      margin-top: 2.8em;
      color: #7efca9;
      font-size: 1.04em;
      text-align: center;
      opacity: 0.85;
      padding-bottom: 2em;
    }
    @media (max-width: 750px) {
      .section { padding: 1em 0.5em; }
      body { padding: 0 0.2em; }
    }
  </style>
</head>
<body>
  <header>
    Llama 2 7B Setup<br>on Pi 5 (16GB) — Tangnet Mesh Guide
  </header>

  <div class="section">
    <h2>1. Prepare Your Pi</h2>
    <ul>
      <li>Start with <b>64-bit Raspberry Pi OS</b> (bookworm or bullseye).</li>
      <li>Connect to your mesh (LAN, WiFi, or Tailscale/ZeroTier).</li>
      <li>Ensure at least <b>16GB free disk space</b> for models, swap, and build files.</li>
      <li>Update all packages:</li>
    </ul>
    <pre>sudo apt update && sudo apt upgrade -y</pre>
  </div>

  <div class="section">
    <h2>2. Install Requirements</h2>
    <ul>
      <li>Install build tools & git:</li>
    </ul>
    <pre>sudo apt install -y build-essential git cmake</pre>
    <ul>
      <li>(Optional, but recommended) <b>htop</b> and <b>screen</b> for monitoring and multitasking:</li>
    </ul>
    <pre>sudo apt install -y htop screen</pre>
  </div>

  <div class="section">
    <h2>3. Get Llama.cpp</h2>
    <ul>
      <li>Clone the repo:</li>
    </ul>
    <pre>
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make -j6
    </pre>
    <div class="tip">
      <b>Tip:</b> <code>-j6</code> uses all Pi 5 cores for faster build!
    </div>
  </div>

  <div class="section">
    <h2>4. Download Llama 2 7B Model (Quantized)</h2>
    <ul>
      <li>On your laptop/desktop, go to <a href="https://huggingface.co/TheBloke/Llama-2-7B-GGUF" target="_blank">TheBloke / Llama-2-7B-GGUF</a>.</li>
      <li>Download a file like <b>llama-2-7b.Q4_K_M.gguf</b> (Q4_K_M is a good Pi balance of quality/speed).</li>
      <li>Transfer the file to your Pi (e.g., to <code>~/models/</code>):</li>
    </ul>
    <pre>
# On your laptop:
scp /path/to/llama-2-7b.Q4_K_M.gguf pi@192.168.1.32:/home/pi/models/
    </pre>
    <div class="tip">
      <b>On Windows?</b> Use <a href="https://winscp.net/" target="_blank">WinSCP</a> or try scp in PowerShell.<br>
      <b>If <code>~/models</code> doesn't exist:</b> <code>mkdir -p ~/models</code> on the Pi first!
    </div>
  </div>

  <div class="section">
    <h2>5. (Optional) Move/Symlink Model to llama.cpp</h2>
    <pre>
mv ~/models/llama-2-7b.Q4_K_M.gguf ~/llama.cpp/models/
# OR symlink:
ln -s ~/models/llama-2-7b.Q4_K_M.gguf ~/llama.cpp/models/
    </pre>
  </div>

  <div class="section">
    <h2>6. Run Llama 2 7B Model!</h2>
    <ul>
      <li>Run a simple prompt:</li>
    </ul>
    <pre>
cd ~/llama.cpp
./main -m models/llama-2-7b.Q4_K_M.gguf -t 8 -n 128 -p "What can I build with a Pi mesh?"
    </pre>
    <ul>
      <li>For interactive chat:</li>
    </ul>
    <pre>
./main -m models/llama-2-7b.Q4_K_M.gguf -t 8 -c 2048 --color -i
    </pre>
    <div class="tip">
      <b>Notes:</b>
      <ul>
        <li><b>-t 8</b>: Use all Pi 5 cores (adjust if you have other tasks running)</li>
        <li><b>-c 2048</b>: Context window; decrease to <b>-c 1024</b> if you run out of memory</li>
        <li>If you get memory errors: try a more aggressive quantization (Q4_0, Q3_K_M) or lower <b>-c</b></li>
      </ul>
    </div>
  </div>

  <div class="section">
    <h2>7. Mesh Tips & Integration</h2>
    <ul>
      <li><b>tangent-node-02</b> can serve as your "beefy model node" for bigger LLM answers, while <b>node-01</b> stays on TinyLlama for fast/always-on Q&A.</li>
      <li>Use <b>SSH</b> or <b>Tailscale</b> to run remote commands or set up APIs for cross-node model serving.</li>
      <li>Add scripts to let node-01 forward "hard" questions to node-02!</li>
      <li>Keep both models up to date using your mesh's automation workflow.</li>
    </ul>
  </div>

  <div class="footer">
    Tangnet Mesh Lab Docs — Llama 2 7B on Pi 5 (16GB)<br>
    &copy; 2025 Brandon Neidlinger<br>
    <span style="color:#7efca9;">“Mesh stronger, mesh smarter!”</span>
  </div>
</body>
</html>
